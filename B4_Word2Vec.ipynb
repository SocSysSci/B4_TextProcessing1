{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SocSysSci/B4_TextProcessing1/blob/main/B4_Word2Vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. テキストデータの収集"
      ],
      "metadata": {
        "id": "2THGQ3KeRz0l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-1. 【授業中は実行しません】　スクレイピング\n",
        "\n",
        "**すごく時間がかかるので授業中はここを実行せず取得済みのテキストデータを利用します！！**\n",
        "\n",
        "ここではYahoo!知恵袋からQ&Aのテキストデータをスクレイピングで取得します。\n",
        "以下のように実行すると指定されたカテゴリの質問と回答のテキストをスクレイピングします。\n",
        "\n",
        "```\n",
        "text = get_yahoo_chiebukuro_text(カテゴリ)\n",
        "```\n",
        "\n",
        "「カテゴリ」としては以下のようなものを登録しています。\n",
        "- 恋愛相談\n",
        "- 大学受験\n",
        "- 住宅\n",
        "\n",
        "Yahoo!知恵袋のカテゴリIDが分かれば\n",
        "\n",
        "なお，過剰な負荷を避けるために以下のような配慮をしています。\n",
        "- 一度に取得する回答の数は最大60？（10ページ分）\n",
        "- １ページ分の回答をスクレイピングするたびにほんの少し休みます。\n",
        "\n",
        "**↓うっかりクリックして実行しないように！！！**"
      ],
      "metadata": {
        "id": "2hT3FGn_Z2aW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxXwwouIh0ZN"
      },
      "outputs": [],
      "source": [
        "# 必要なパッケージの読み込み\n",
        "import requests\n",
        "import re\n",
        "import time\n",
        "import random\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAIN_SORT_KEY = {\"更新日時の新しい順\":16, \"更新日時の古い順\": 17, \"質問日時の新しい順\":20, \"質問日時の古い順\":21, \"回答数の多い順\":4, \"回答数の少ない順\":5, \"お礼の多い順\":8}\n",
        "DETAIL_SORT_KEY={\"ナイスの多い順\":0, \"新しい順\":1, \"古い順\":2}\n",
        "QA_STAT = {\"回答受付中\": 0, \"解決済み\": 1, \"全て\": 3}\n",
        "\n",
        "def get_qa_text(base_link, sort_mode=\"ナイスの多い順\"):\n",
        "  text_list = []\n",
        "  sort_id = DETAIL_SORT_KEY[sort_mode]\n",
        "  link = base_link + \"?sort={}\".format(sort_id)\n",
        "\n",
        "  res = requests.get(link)\n",
        "  soup = BeautifulSoup(res.text.encode(res.encoding), \"html.parser\")\n",
        "\n",
        "  ans_count_tag = soup.select_one('strong[class*=\"Chie-QuestionItem__AnswerNumber__\"]')\n",
        "  ans_count = int(\"\".join(ans_count_tag.text.split(\",\"))) if ans_count_tag is not None else 0\n",
        "  max_page = min(10, (ans_count // 10 + 1) if 0 < ans_count else 1)\n",
        "\n",
        "  print(f\"*** QA: # of answers:{ans_count}, pages:{max_page} ... \")\n",
        "\n",
        "  print(\"**** extracting Question ... \", end = \"\")\n",
        "  #que_text_tag = soup.select_one('div[class*=\"Chie-QuestionItem__Text__\"]')\n",
        "  que_text_tag = soup.find(\"div\", class_=re.compile(r\"Chie-QuestionItem__Text__\"))\n",
        "  if que_text_tag is not None:\n",
        "    text_list.append(que_text_tag.text)\n",
        "  print(\"done.\")\n",
        "\n",
        "  for page_num in range(1, max_page+1):\n",
        "    print(f\"**** extracting Answers in page {page_num} ... \", end=\"\")\n",
        "    page_link = link + \"&page={}\".format(page_num)\n",
        "\n",
        "    # 過剰な負荷を避けるため，１ページ分の記事を取得する前に 1〜3秒休む\n",
        "    time.sleep(random.randrange(1, 3))\n",
        "\n",
        "    res = requests.get(link)\n",
        "    soup = BeautifulSoup(res.text.encode(res.encoding), \"html.parser\")\n",
        "\n",
        "    # ans_text_list = soup.select('div[class*=\"Chie-AnswerItem__ItemText__\"]')\n",
        "    ans_text_list = soup.find_all(\"div\", class_=re.compile(r\"Chie-AnswerItem__ItemText__\"))\n",
        "    ans_text_num = 0\n",
        "    for ans_tag in ans_text_list:\n",
        "      text_list.append(ans_tag.text)\n",
        "      ans_text_num += 1\n",
        "    print(f\"{ans_text_num} answers extracted.\")\n",
        "\n",
        "  return \"\".join(text_list)\n",
        "\n",
        "\n",
        "def get_qa_link_list(url):\n",
        "  qa_link_list = []\n",
        "\n",
        "  # 過剰な負荷を避けるため，１ページ分のリンクリストを取得する前に 1〜3秒休む\n",
        "  time.sleep(random.randrange(1, 3))\n",
        "\n",
        "  res = requests.get(url)\n",
        "  soup = BeautifulSoup(res.text.encode(res.encoding), \"html.parser\")\n",
        "  qa_list = soup.find(\"div\", {\"id\": \"qa_lst\"})\n",
        "  qa_atag_list = qa_list.find_all(\"a\")\n",
        "  for qa_atag in qa_atag_list:\n",
        "    qa_link_list.append(qa_atag[\"href\"])\n",
        "  return qa_link_list\n",
        "\n",
        "\n",
        "def get_yahoo_chiebukuro_text(category_id, sort_mode=\"回答数の多い順\", qa_stat=\"解決済み\", max_qa_num = 10, page_count=1):\n",
        "  base_url = \"https://chiebukuro.yahoo.co.jp/category/{}/question/list?flg={}&sort={}&page={}\"\n",
        "  sort_id = MAIN_SORT_KEY[sort_mode]\n",
        "  qa_stat_id = QA_STAT[qa_stat]\n",
        "\n",
        "  print(\"* launching the process ... \")\n",
        "  qa_link_list = []\n",
        "  for page in range(1, page_count+1):\n",
        "    print(f\"** retrieving QA links from page #{page} ... \", end = \"\")\n",
        "    url = base_url.format(category_id, qa_stat_id, sort_id, page)\n",
        "    qa_link_list_sub = get_qa_link_list(url)\n",
        "    qa_link_list.extend(qa_link_list_sub)\n",
        "    print(f\"{len(qa_link_list_sub)} links retrieved.\")\n",
        "    if max_qa_num < len(qa_link_list):\n",
        "      break\n",
        "\n",
        "  # 指定された個数までのQ&Aしか取得しない（デフォルトでは10）\n",
        "  qa_link_list = qa_link_list[:max_qa_num]\n",
        "\n",
        "  text_list = []\n",
        "  for qa_link in qa_link_list:\n",
        "    print(f\"** retrieving QA from {qa_link} ... \")\n",
        "    text_list.append(get_qa_text(qa_link))\n",
        "    print(\"** retrieving QA done.\")\n",
        "  print(\"* done.\")\n",
        "\n",
        "  return \"\".join(text_list)"
      ],
      "metadata": {
        "id": "HO_MCqC2iHbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 誤って実行しないようにコメントアウトしています。実行したい場合は行頭の # を取り除いてください。\n",
        "# CATEGORY = {\"恋愛相談\": 2078675272, \"大学受験\": 2079405665, \"住宅\": 2078297940, \"話題の人物\":2078297579}\n",
        "# text = get_yahoo_chiebukuro_text(CATEGORY[\"話題の人物\"])"
      ],
      "metadata": {
        "id": "vVWR3gFfbPdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-1. 【授業中はこちらを実行】 テキストデータのアップロード\n",
        "\n",
        "テキストデータをスクレイピングで取得する代わりに，取得済みのテキストデータをアップロードして利用します。\n",
        "\n",
        "先週の資料に従って，BEEF+からテキストデータをダウンロードし，Google Colaboratoryのセッションストレージにアップロードしてください。"
      ],
      "metadata": {
        "id": "_rLQKIKMleW_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 以下では大学受験の記事（yc_univ_2018.txt)を利用\n",
        "with open(\"yc_univ_2018.txt\", \"r\") as f:\n",
        "  text = f.read()"
      ],
      "metadata": {
        "id": "nPyc0KtGn4Ek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-2. テキストの確認\n",
        "\n",
        "取得した（または読み込んだ）テキストデータを確認します。\n"
      ],
      "metadata": {
        "id": "m83ebpu5qPWZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "id": "15hCvv-bqD06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. クレンジング\n",
        "\n",
        "テキストデータに含まれる不要なデータを取り除きます。\n",
        "\n",
        "上記で確認したように，今回のテキストデータはスクレイピングの際にHTMLタグなどは除去しているので，あまり不要なデータがありません。\n",
        "\n",
        "今回は改行コードやタブ文字などの余分な空白文字を取り除くという処理だけ行います。場合によってはアルファベットの大文字小文字や全角半角の変換も必要かもしれません。"
      ],
      "metadata": {
        "id": "D4qBCXwjqiYK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2-1. クレンジング用の関数定義"
      ],
      "metadata": {
        "id": "wvW5aydduMea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# クレンジング用の関数定義\n",
        "def cleansing(text):\n",
        "  clean_text = re.sub(\"\\s\", \"\", text)  # 余分な空白の除去\n",
        "  return clean_text"
      ],
      "metadata": {
        "id": "sLK6yd3Ct1g3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2-2. クレンジングの実行"
      ],
      "metadata": {
        "id": "z3cnLRD4uWgG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clean_text = cleansing(text)"
      ],
      "metadata": {
        "id": "HlUSV_-fuHv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2-3. テキストの確認\n",
        "\n",
        "クレンジング後のテキストを確認します"
      ],
      "metadata": {
        "id": "xLtAZZmLufQR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clean_text"
      ],
      "metadata": {
        "id": "5vo20gM2uyw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. トークン化\n",
        "\n",
        "以下では形態素解析パッケージを利用して，テキストデータをトークン（ここでは形態素）に分割，分ち書きのテキスト（空白でトークンを区切ったテキスト）に変換します。\n"
      ],
      "metadata": {
        "id": "1ofnQqhZu54F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3-1. 形態素パッケージのインストール\n",
        "\n",
        "作業に先立って授業資料の［補足1］を見てください。"
      ],
      "metadata": {
        "id": "AsTD955rvMcY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install janome"
      ],
      "metadata": {
        "id": "TRhXi8qixDpd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3-2. トークン化用の関数定義\n"
      ],
      "metadata": {
        "id": "mLYRPFDkxIc7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from janome.tokenizer import Tokenizer\n",
        "\n",
        "def wakati(text, accept_pos=[\"名詞\", \"動詞\"]):\n",
        "  punctuations = [\"。\", \".\", \".\", \"？\", \"?\", \"！\", \"!\"]\n",
        "  tokenizer = Tokenizer()\n",
        "  tokens = tokenizer.tokenize(text)\n",
        "  sentence_list = []\n",
        "  token_list = []\n",
        "  for token in tokens:\n",
        "    pos = token.part_of_speech.split(\",\")[0]\n",
        "    if token.surface in punctuations and 0 < len(token_list):\n",
        "      sentence_list.append(\" \".join(token_list)) # トークンを空白で繋げる\n",
        "      token_list = []\n",
        "    elif len(accept_pos) == 0 or pos in accept_pos:\n",
        "      token_list.append(token.surface)\n",
        "  if 0 < len(token_list):\n",
        "    sentence_list.append(\" \".join(token_list)) # トークンを空白で繋げる\n",
        "  return \"\\n\".join(sentence_list) # センテンスごとに改行で分ける"
      ],
      "metadata": {
        "id": "ImtD1aT8xHjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3-3. トークン化してみる\n",
        "\n",
        "Janomeが遅いのですごく時間がかかる可能性があります。"
      ],
      "metadata": {
        "id": "goRaWu1p236X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wakati_text = wakati(clean_text, accept_pos=[])"
      ],
      "metadata": {
        "id": "cGYBOLAZ0uCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3-4. テキストの確認\n",
        "\n",
        "分ち書きされたテキストデータを確認します。"
      ],
      "metadata": {
        "id": "0Q_X8uBz1iJ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wakati_text"
      ],
      "metadata": {
        "id": "BtXkGA4q1wui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3-5. テキストの保存\n",
        "\n",
        "一旦，分ち書きされたテキストデータを保存します。\n",
        "\n",
        "保存したあと更新してみてください。うまく保存されている場合，左のストレージペインにファイルが表示されます。"
      ],
      "metadata": {
        "id": "czuRS-P-40hx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"yc_univ_2018_wakati.txt\", \"w\") as f:\n",
        "  f.write(wakati_text)"
      ],
      "metadata": {
        "id": "AdQRj0r141GP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Word2Vecしてみる\n",
        "\n",
        "保存された分ち書きテキストデータを使ってWord2Vecを学習させてみます。"
      ],
      "metadata": {
        "id": "vpkTjWDu3DQu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4-1. Word2Vecを学習する"
      ],
      "metadata": {
        "id": "yK87Hk3a3lsq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import word2vec\n",
        "\n",
        "# 保存された分ち書きテキストのファイル名を指定\n",
        "sentences = word2vec.LineSentence(\"yc_univ_wakati.txt\")\n",
        "\n",
        "# Word2Vecを学習\n",
        "# size：ベクトルの次元（トークンの数にもよるが100〜300くらい）\n",
        "# min_count：最小出現回数\n",
        "# window：どれぐらい周辺の単語を考慮するか\n",
        "model = word2vec.Word2Vec(sentences, vector_size=100, min_count=10, window=10)"
      ],
      "metadata": {
        "id": "ipKGVeR83Mvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4-2. Word2Vecのテスト"
      ],
      "metadata": {
        "id": "Y41GwN8bMlvL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar(positive=\"合格\", topn=10, restrict_vocab=10000)"
      ],
      "metadata": {
        "id": "9IqpKX-E5l70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "v = model.wv[\"大学\"] - model.wv[\"高校\"]\n",
        "model.wv.most_similar(positive=[v], topn=10)"
      ],
      "metadata": {
        "id": "2zmNOays6Qn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4-3. 学習済みモデルの保存\n",
        "\n",
        "あとで読み込んで再利用できるように学習済みモデルを保存します。"
      ],
      "metadata": {
        "id": "R_UmlcxTPe6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"yc_univ_2018.model\")"
      ],
      "metadata": {
        "id": "S6pYAiW6Pm44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. トークン空間をプロットしてみる\n",
        "\n",
        "PCAを使ってトークン・ベクトルの次元を低次元化し，プロットしてみる。"
      ],
      "metadata": {
        "id": "jnbcZQ7SPuDj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1 日本語プロット用パッケージのインストール"
      ],
      "metadata": {
        "id": "4FFuWm5fUV5w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install japanize-matplotlib"
      ],
      "metadata": {
        "id": "b2GoKkBXUdeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2 プロット用の関数定義"
      ],
      "metadata": {
        "id": "fOlaMr9RSgwI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "import japanize_matplotlib\n",
        "\n",
        "def get_converter(x):\n",
        "    converter = PCA(random_state=0)\n",
        "    converter.fit(x)\n",
        "    return converter\n",
        "\n",
        "\n",
        "def make_plot(model, pca=(0,1), num = 200, min_word_len = 4, width = 10, height = 10, font_size = 14):\n",
        "    vocab = model.wv.index_to_key\n",
        "    emb_tuple = tuple([model.wv[v] for v in vocab])\n",
        "    converter = get_converter(np.vstack(emb_tuple))\n",
        "\n",
        "    plt.rcParams[\"font.size\"] = font_size\n",
        "    fig = plt.figure(figsize=(width, height))  # 図のサイズ\n",
        "    cmap = [\"red\", \"blue\", \"green\", \"magenta\", \"black\", \"cyan\", \"yellow\"]\n",
        "\n",
        "    for i, v in enumerate([vocab]):\n",
        "        available_vocab = []\n",
        "        orig_pos = []\n",
        "        for j in range(len(v)):\n",
        "            if min_word_len <= len(v[j]) and len(available_vocab) < num:\n",
        "                p = model.wv[v[j]]\n",
        "                available_vocab.append(v[j])\n",
        "                orig_pos.append(p)\n",
        "\n",
        "        if 0 < len(orig_pos):\n",
        "            emb_pos = converter.transform(orig_pos)\n",
        "            marker = [2 for _ in range(len(emb_pos))]\n",
        "            plt.scatter(emb_pos[:, pca[0]], emb_pos[:, pca[1]], s=marker, c=cmap[i % len(cmap)])\n",
        "            for label, x, y in zip(available_vocab, emb_pos[:, pca[0]], emb_pos[:, pca[1]]):\n",
        "                plt.annotate(label, xy=(x, y), xytext=(3, -2), textcoords='offset points')\n",
        "    return fig"
      ],
      "metadata": {
        "id": "NyLNvKXXSWbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.3 プロットの実行"
      ],
      "metadata": {
        "id": "8magdKzr4Coi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = make_plot(model, pca=(0,1), num = 100, min_word_len = 2, font_size=12)"
      ],
      "metadata": {
        "id": "xJwGQZMFUFuw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}